{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Science Wednesday 03/13/2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "% matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About Data\n",
    "As an example for text classification we work with 1956 comments from 5 different YouTube videos. The comments were collected via the YouTube API from five of the ten most viewed videos on YouTube in the first half of 2015. The comments were manually labeled as spam or legitimate. Spam was coded with a “1” and legitimate comments with a “0”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>COMMENT_ID</th>\n",
       "      <th>AUTHOR</th>\n",
       "      <th>DATE</th>\n",
       "      <th>CONTENT</th>\n",
       "      <th>CLASS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LZQPQhLyRh80UYxNuaDWhIGQYNQ96IuCg-AYWqNPjpU</td>\n",
       "      <td>Julius NM</td>\n",
       "      <td>2013-11-07T06:20:48</td>\n",
       "      <td>Huh, anyway check out this you[tube] channel: ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LZQPQhLyRh_C2cTtd9MvFRJedxydaVW-2sNg5Diuo4A</td>\n",
       "      <td>adam riyati</td>\n",
       "      <td>2013-11-07T12:37:15</td>\n",
       "      <td>Hey guys check out my new channel and our firs...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LZQPQhLyRh9MSZYnf8djyk0gEF9BHDPYrrK-qCczIY8</td>\n",
       "      <td>Evgeny Murashkin</td>\n",
       "      <td>2013-11-08T17:34:21</td>\n",
       "      <td>just for test I have to say murdev.com</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>z13jhp0bxqncu512g22wvzkasxmvvzjaz04</td>\n",
       "      <td>ElNino Melendez</td>\n",
       "      <td>2013-11-09T08:28:43</td>\n",
       "      <td>me shaking my sexy ass on my channel enjoy ^_^ ﻿</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>z13fwbwp1oujthgqj04chlngpvzmtt3r3dw</td>\n",
       "      <td>GsMega</td>\n",
       "      <td>2013-11-10T16:05:38</td>\n",
       "      <td>watch?v=vtaRGgvGtWQ   Check this out .﻿</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    COMMENT_ID            AUTHOR  \\\n",
       "0  LZQPQhLyRh80UYxNuaDWhIGQYNQ96IuCg-AYWqNPjpU         Julius NM   \n",
       "1  LZQPQhLyRh_C2cTtd9MvFRJedxydaVW-2sNg5Diuo4A       adam riyati   \n",
       "2  LZQPQhLyRh9MSZYnf8djyk0gEF9BHDPYrrK-qCczIY8  Evgeny Murashkin   \n",
       "3          z13jhp0bxqncu512g22wvzkasxmvvzjaz04   ElNino Melendez   \n",
       "4          z13fwbwp1oujthgqj04chlngpvzmtt3r3dw            GsMega   \n",
       "\n",
       "                  DATE                                            CONTENT  \\\n",
       "0  2013-11-07T06:20:48  Huh, anyway check out this you[tube] channel: ...   \n",
       "1  2013-11-07T12:37:15  Hey guys check out my new channel and our firs...   \n",
       "2  2013-11-08T17:34:21             just for test I have to say murdev.com   \n",
       "3  2013-11-09T08:28:43   me shaking my sexy ass on my channel enjoy ^_^ ﻿   \n",
       "4  2013-11-10T16:05:38            watch?v=vtaRGgvGtWQ   Check this out .﻿   \n",
       "\n",
       "   CLASS  \n",
       "0      1  \n",
       "1      1  \n",
       "2      1  \n",
       "3      1  \n",
       "4      1  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Read in the data\n",
    "train_data = []\n",
    "data_files = ['Youtube01-Psy.csv','Youtube02-KatyPerry.csv','Youtube03-LMFAO.csv','Youtube04-Eminem.csv','Youtube05-Shakira.csv']\n",
    "for file in data_files:\n",
    "    data = pd.read_csv(file)\n",
    "    train_data.append(data)\n",
    "train_data = pd.concat(train_data)\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    1005\n",
       "0     951\n",
       "Name: CLASS, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check the count of each class\n",
    "train_data['CLASS'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning the data\n",
    "1. Drop insignificant columns\n",
    "2. Process the contents of data\n",
    "3. Extract features from the data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop columns\n",
    "def drop_features(features,data):\n",
    "    data.drop(features,axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 1956 entries, 0 to 369\n",
      "Data columns (total 2 columns):\n",
      "CONTENT    1956 non-null object\n",
      "CLASS      1956 non-null int64\n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 45.8+ KB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CONTENT</th>\n",
       "      <th>CLASS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Huh, anyway check out this you[tube] channel: ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Hey guys check out my new channel and our firs...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>just for test I have to say murdev.com</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>me shaking my sexy ass on my channel enjoy ^_^ ﻿</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>watch?v=vtaRGgvGtWQ   Check this out .﻿</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             CONTENT  CLASS\n",
       "0  Huh, anyway check out this you[tube] channel: ...      1\n",
       "1  Hey guys check out my new channel and our firs...      1\n",
       "2             just for test I have to say murdev.com      1\n",
       "3   me shaking my sexy ass on my channel enjoy ^_^ ﻿      1\n",
       "4            watch?v=vtaRGgvGtWQ   Check this out .﻿      1"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "drop_features(['COMMENT_ID','AUTHOR','DATE'],train_data)\n",
    "train_data.info()\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#process content\n",
    "def process_content(content):\n",
    "    return \" \".join(re.findall(\"[A-Za-z]+\",content.lower()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CONTENT</th>\n",
       "      <th>CLASS</th>\n",
       "      <th>processed_content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Huh, anyway check out this you[tube] channel: ...</td>\n",
       "      <td>1</td>\n",
       "      <td>huh anyway check out this you tube channel kob...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Hey guys check out my new channel and our firs...</td>\n",
       "      <td>1</td>\n",
       "      <td>hey guys check out my new channel and our firs...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>just for test I have to say murdev.com</td>\n",
       "      <td>1</td>\n",
       "      <td>just for test i have to say murdev com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>me shaking my sexy ass on my channel enjoy ^_^ ﻿</td>\n",
       "      <td>1</td>\n",
       "      <td>me shaking my sexy ass on my channel enjoy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>watch?v=vtaRGgvGtWQ   Check this out .﻿</td>\n",
       "      <td>1</td>\n",
       "      <td>watch v vtarggvgtwq check this out</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             CONTENT  CLASS  \\\n",
       "0  Huh, anyway check out this you[tube] channel: ...      1   \n",
       "1  Hey guys check out my new channel and our firs...      1   \n",
       "2             just for test I have to say murdev.com      1   \n",
       "3   me shaking my sexy ass on my channel enjoy ^_^ ﻿      1   \n",
       "4            watch?v=vtaRGgvGtWQ   Check this out .﻿      1   \n",
       "\n",
       "                                   processed_content  \n",
       "0  huh anyway check out this you tube channel kob...  \n",
       "1  hey guys check out my new channel and our firs...  \n",
       "2             just for test i have to say murdev com  \n",
       "3         me shaking my sexy ass on my channel enjoy  \n",
       "4                 watch v vtarggvgtwq check this out  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data['processed_content'] = train_data['CONTENT'].apply(process_content)\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop 'CONTENT', lets use 'PROCESSED CONTENT'\n",
    "drop_features(['CONTENT'],train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train test split\n",
    "x_train, x_test, y_train, y_test = train_test_split(train_data['processed_content'],train_data['CLASS'],test_size=0.2,random_state=57)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feature extraction\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count_vect = CountVectorizer(stop_words='english')\n",
    "x_train_counts = count_vect.fit_transform(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 918)\t2\n",
      "  (0, 525)\t1\n",
      "  (0, 1202)\t1\n",
      "  (0, 2229)\t1\n",
      "  (0, 1376)\t1\n",
      "  (0, 1372)\t1\n",
      "  (0, 1285)\t1\n",
      "  (0, 2105)\t1\n",
      "  (0, 498)\t1\n",
      "  (0, 2026)\t1\n",
      "  (0, 2416)\t1\n",
      "  (0, 467)\t1\n",
      "  (0, 155)\t1\n",
      "  (0, 446)\t1\n",
      "  (0, 451)\t1\n",
      "  (0, 3108)\t1\n",
      "  (0, 2618)\t1\n",
      "  (0, 463)\t1\n",
      "  (0, 1726)\t1\n",
      "  (0, 3107)\t1\n",
      "  (0, 579)\t1\n",
      "  (1, 447)\t1\n",
      "  (1, 2802)\t1\n",
      "  (2, 312)\t1\n",
      "  (2, 1190)\t1\n",
      "  :\t:\n",
      "  (1559, 2802)\t1\n",
      "  (1560, 878)\t1\n",
      "  (1560, 3234)\t1\n",
      "  (1560, 1197)\t1\n",
      "  (1560, 2480)\t1\n",
      "  (1560, 1303)\t1\n",
      "  (1560, 2695)\t1\n",
      "  (1560, 1756)\t1\n",
      "  (1561, 1293)\t1\n",
      "  (1561, 2218)\t1\n",
      "  (1561, 3320)\t1\n",
      "  (1561, 463)\t1\n",
      "  (1562, 353)\t1\n",
      "  (1562, 2925)\t1\n",
      "  (1562, 766)\t1\n",
      "  (1562, 2722)\t1\n",
      "  (1562, 3039)\t1\n",
      "  (1562, 351)\t4\n",
      "  (1562, 3186)\t1\n",
      "  (1562, 2168)\t1\n",
      "  (1562, 2802)\t1\n",
      "  (1563, 2187)\t1\n",
      "  (1563, 534)\t1\n",
      "  (1563, 3180)\t1\n",
      "  (1563, 1689)\t1\n"
     ]
    }
   ],
   "source": [
    "print(x_train_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tranformer = TfidfTransformer()\n",
    "x_train_tfidf = tranformer.fit_transform(x_train_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 579)\t0.1817627430483752\n",
      "  (0, 3107)\t0.09954971942502731\n",
      "  (0, 1726)\t0.264551204493268\n",
      "  (0, 463)\t0.0836688831218715\n",
      "  (0, 2618)\t0.22662109881473988\n",
      "  (0, 3108)\t0.15012737481999192\n",
      "  (0, 451)\t0.11642107551984253\n",
      "  (0, 446)\t0.1722249653649856\n",
      "  (0, 155)\t0.21668874003588182\n",
      "  (0, 467)\t0.25055233104343294\n",
      "  (0, 2416)\t0.22129897413383598\n",
      "  (0, 2026)\t0.22129897413383598\n",
      "  (0, 498)\t0.2089846040361916\n",
      "  (0, 2105)\t0.22129897413383598\n",
      "  (0, 1285)\t0.22129897413383598\n",
      "  (0, 1372)\t0.1722249653649856\n",
      "  (0, 1376)\t0.14331344758515127\n",
      "  (0, 2229)\t0.19498573058635657\n",
      "  (0, 1202)\t0.1817627430483752\n",
      "  (0, 525)\t0.11157517176922856\n",
      "  (0, 918)\t0.529102408986536\n",
      "  (1, 2802)\t0.4169027916259167\n",
      "  (1, 447)\t0.9089510780754471\n",
      "  (2, 463)\t0.06352827648147828\n",
      "  (2, 2802)\t0.0837971152121541\n",
      "  :\t:\n",
      "  (1559, 2794)\t0.4133403828985252\n",
      "  (1560, 1756)\t0.22724886827505128\n",
      "  (1560, 2695)\t0.20333946164509845\n",
      "  (1560, 1303)\t0.3569460101522803\n",
      "  (1560, 2480)\t0.3705517204574569\n",
      "  (1560, 1197)\t0.4039422081563246\n",
      "  (1560, 3234)\t0.4574005489887746\n",
      "  (1560, 878)\t0.5195261434832101\n",
      "  (1561, 463)\t0.24347592019084308\n",
      "  (1561, 3320)\t0.3112672914461533\n",
      "  (1561, 2218)\t0.5011735588844561\n",
      "  (1561, 1293)\t0.769842330245672\n",
      "  (1562, 2802)\t0.13681556781506937\n",
      "  (1562, 2168)\t0.1783064258161094\n",
      "  (1562, 3186)\t0.20424935969292815\n",
      "  (1562, 351)\t0.7106511867563018\n",
      "  (1562, 3039)\t0.2549946814174879\n",
      "  (1562, 2722)\t0.31060468225562\n",
      "  (1562, 766)\t0.2417198859852419\n",
      "  (1562, 2925)\t0.2743398844388296\n",
      "  (1562, 353)\t0.3279588039343718\n",
      "  (1563, 1689)\t0.30165641960882134\n",
      "  (1563, 3180)\t0.4618319512460967\n",
      "  (1563, 534)\t0.40660849174513813\n",
      "  (1563, 2187)\t0.728274802363609\n"
     ]
    }
   ],
   "source": [
    "print(x_train_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test_counts = count_vect.transform(x_test)\n",
    "x_test_tfidf = tranformer.transform(x_test_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ML Modeling\n",
    "We have completed cleaning of our data and we have training samples to train the model and testing samples to verify the accuracy of our models. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression\n",
    "Logistic regression is a simple statistical model which predicts a binary response (ex. 0/1). For this data, we are predicting whether a comment is a spam or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Create and train Logistic Regression model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "model_reg = LogisticRegression()\n",
    "model_reg.fit(x_train_tfidf,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9260204081632653\n"
     ]
    }
   ],
   "source": [
    "#Run it on test data and calculate the accuracy\n",
    "accuracy = model_reg.score(x_test_tfidf, y_test)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest Classifier\n",
    "Random Forest is a supervised learning algorithm. Like you can already see from it’s name, it creates a forest and makes it somehow random. The forest it builds, is an ensemble of Decision Trees. To say it in simple words: Random forest builds multiple decision trees and merges them together to get a more accurate and stable prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Create and train Random Forest Classifier model\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "model_rfc = RandomForestClassifier()\n",
    "model_rfc.fit(x_train_tfidf,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9311224489795918\n"
     ]
    }
   ],
   "source": [
    "#Run it on test data and calculate the accuracy\n",
    "accuracy = model_rfc.score(x_test_tfidf, y_test)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Machine\n",
    "The objective of the support vector machine algorithm is to find a hyperplane in an N-dimensional space(N — the number of features) that distinctly classifies the data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=100, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma=1, kernel='rbf',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Create and train Support Vector Machine model\n",
    "from sklearn import svm\n",
    "model_svm = svm.SVC(C=100, gamma = 1)\n",
    "model_svm.fit(x_train_tfidf, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9336734693877551\n"
     ]
    }
   ],
   "source": [
    "#Run it on test data and calculate the accuracy\n",
    "accuracy = model_svm.score(x_test_tfidf, y_test)\n",
    "print(accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
